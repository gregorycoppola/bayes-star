\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[authoryear]{natbib}
\usepackage{amsmath}
\usepackage{hyperref}
\title{\bf Symbolic Logic is Also Needed}
\author{
    Greg Coppola \\
    {\em coppola.ai}
}
\date{\today}

\begin{document}
\maketitle


\begin{abstract}
Here we present the {\bf Quantified Bayesian Network} ({\bf QBN}).
This is a data structure that, we argue, can end the problem of {\em LLM hallucination}.
The {\bf QBN} provides a {\bf generative model} of the {\bf logical structure} behind the sentences in natural language.
If the {\bf world knowledge} of the LLM can be transferred into the QBN, we argue that that will constitute {\bf Artificial General Intelligence}.
The first insight behind the QBN is to marry {\bf Bayesian Networks} with {\bf First-Order Logic} in an efficient way, to obtain the benefits of both {\bf probabilitic} and {\bf logical} reasoning.
The second insight behind the QBN is to define a logical calculus directly over the {\bf semantic role labeling} representation of an idea, which greatly simplifies the {\bf implementation} and {\bf learning}.
The QBN by itself is {\em not} AGI, but we outline {\bf combined generative model} of logic and syntax that might be able to populate the {\bf QBN} from large-scale unlabeled text data.
\end{abstract}


\tableofcontents

\section{Introduction}
The {\bf Large Language Model} ({\bf LLM}) is the hottest data structure in artificial intelligence and computer science right now.
Developed in \cite{vaswani2017attention, devlin2018bert, radford2018improving, brown2020language}, the LLM has sparked a dynamic industrial effort and competition to create {\em artificially intelligent assistants}, ``chat bots,'' or simply {\em LLM}'s.
Noted Silivon Valley investor Jason Calacanis recently said of the LLM-based technology boom:
\begin{quote}
    AI is the opportunity of our lifetimes... 
    bigger than the internet, PCs, mobile, and cloud -- combined.  
    \cite{Jason2023}
\end{quote}
But, there are some major, well-known limitations with LLM's that limit their applicability.
The main problem is {\bf hallucinations}: the LLM does not {\em know what it does not know}, and this lack of reliability limits the ability of the system to be used fully automatically, i.e., without a human ``in the loop.''
Another noted problem is that the LLM does not {\bf reason logically}, and does not have a {\bf logically consistent world view} \cite{hinton:cbs:2023, steedman:2022}.
We argue that these two problems are causally related.
\section{Limitations of LLM's}
\subsection{Hallucinations}
The model is ``hallucinating'' when it is answering a question with an answer completely unsuported by the training data.
This is the most commonly listed problem with LLM's.
\cite{sutskever:huang:2023} has said this limits the usefulness of LLM's because there must always be a human ``in the loop'', and so things cannot be fully automated.
\subsection{Cannot Do Reasoning}
It is known that LLM's cannot do reasoning.
\cite{hinton:cbs:2023} has underlined the problem that LLM's 1) do not have a ``logically consistent worldview'' and 2) do not understand how different ``worldviews'' (what we call {\em theories}) lead to different conclusions.
\cite{steedman:2022} has suggested that LLM's are not doing reasoning, and the field should not lose sight of the necessity of symbolic logic.
\subsection{Our Analysis}
These two problems are related.
In order to not hallucinate, a model must be able to explain {\em why} it believes what it does.
But the ability to answer {\em why} implies an understanding of {\em causality}.
And, {\em causality} is just another way of saying {\em logical inference}, because you cannot have one without the other.
\section{Existing Approaches to Hallucination}
\subsection{Fine-Tuning}
{\em Fine-Tuning} is the strategy that {\em ChatGPT} originally used to constrain the generative model\cite{radford2018improving,radford2019language,brown2020language},
and is also used in the competing products.
Fine-tuning is a process by which access to the activation outputs of the underlying LLM generative model are used
to train a {\em discriminative} model to achieve certain tasks.
This gives an option to reduce many kinds of LLM erros, but not all, because hallucination remains an unsolved problem.
Also, we aesthetically object to the {\em necessity} of a discriminative model in order to not hallucinate.
If the spirit of the LLM is to be {\em generative}, we should push for a generative model that can avoid hallucinations on its own.
\subsection{Retrieval Augmented Generation}
{\em Retrieval Augmented Generation} is a process by which access to an extrinsic traditional {\em discrete} database-based information
product, especially a {\em search engine} \cite{lewis2013combined:2020}.
The idea is that a discriminative or even programmatic process can be created that 1) uses the LLM activations as features, and 2) has access to the discrete outputs of the traditional search engine.
This way, the content of the answer can be checked against the web page.
One limitation of this is that {\em it does not let the LLM make its own theory}.
We are limited to agreeing with the web page.
Also, as with fine-tuning, we propose it is an aesthetic problem that the {\em generative} part of the model cannot stop hallucinating by itself.
RAG can also be referred to as {\em trusted sources}.
\subsection{Vector Databases}
{\em Vector Databases} have been proposed as a solution to hallucinations.
However, vectors do not have any relation to logical calculus that is established.
Thus, there is no concept of {\em true} or {\em false} with vectors in a vector database any more than there is truth or falsity in the LLM.
Also, empirically, this approach has not been gotten to work, despite being suggested as early as {\em ChatGPT}'s hallucination issue was realized.
\section{Background On Logic}
Our proposal is that, in order to eliminate hallucinations, the generative model must model {\em logical relationships} and the logical {\em reasoning process}.
{\em Formal Logic} is the rigorous mathematical study of logic itself.
Logic comes in a a few forms, varying in their levels of complexity, and scope.

\subsection{Propositional Logic}
The most basic logic is {\em propositional logic}.
Propositional logic deals with statements like $A$, $\neg A$, $A \wedge B$, $A \rightarrow B$.
This is limited because it does not allow different propositions to ``share'' any of their ``meanings''.
That is, in the propositional calculus, if $A$ represents the sentence {\em John runs} and $B$ represents
the sentence {\em Mary runs}, there is no way to represents that both sentences share something
in common, because $A$ and $B$ are different letters, with different indices, sharing no reusable parts.

\subsection{First-Order Logic}
\subsubsection{Compositional Structure}
First-order logic solves this problem of sharing structure between similar sentences by
giving us {\em predicates}, {\em entities} and {\em quantifiers}.
Thus, we can say that {\em John runs} as $run(john)$ and {\em Mary runs} as $run(mary)$,
and we see that both expression share some part of the structure, $run$, corresponding
to our idea that the surface forms {\em John runs} and {\em Mary runs} are related.
\subsubsection{Quantification}
First-Order Logic also makes use of {\em quantifiers}.
The sentence $\forall x, late(x)\rightarrow runs(x)$ says that {\em for any person, if they are late, they are running}.
This kind of sentence allows us to identify {\em patterns} between facts, e.g., being late and running.
The problem with First-Order Logic is that it is too {\em deterministic}.
In real-life scenarios, we exploit patterns that are true {\em most} but not {\em all} of the time.
Thus, it might be that being late {\em tends to lead} to running, but it doesn't have to. Also, the data might actually say that being late leads to {\em less} to running, in some possible data set.

\subsection{Beyond First-Order Logic}
First-order logic is not enough to model all human natural language.
This is not to say that it should be thrown out, but that there are certain ``extensions'' to first-order logic that are needed to model the most sophisticated examples of natural language.
\subsubsection{Intensional Logic}
First of all, {\em intensional logic} is needed in order to allow the ``concept'' of a sentence to be the argument to a predicate, rather than simply its truth value (one of {\em true} or {\em false}).
Intensional logics are less well-agreed upon that first-order logic.
But, in any case, they use the same mechanism of universal quantification, only over a specially constructed entity set, based on the logical language.
\subsubsection{Compositional Semantics}
Second, first-order logic by itself does not tell us how to build up the meanings of complicated expressions, e.g. {\em the Mayor of New York}, {\em the Mayor of New Delhi}, from their simpler parts.
This is again simply an area where there is less clarity than first-order logic, only because there are more options, but there are many options to choose from.
\subsection{Conclusion}
In the following work, we will focus on the first-order logic.
First, because all other logics are extensions of the first-order logic, so we can reuse the same techniques for training a first-order system.
Second, for simplicity.
Third, because there is not a consensus on the details of the more complicated extended logics.

\section{Probabilistic Graphical Model}
\subsection{Introduction to Graphical Models}
Graphical models are a marriage between probability theory and graph theory. They are used to represent complex distributions in a compact and intuitive manner. A graphical model is typically represented by a graph, where nodes represent random variables and edges represent probabilistic dependencies between these variables.
The joint probability distribution of a set of random variables 
\[ \{X_1, X_2, ..., X_n\} \]
in a graphical model can be factorized as follows:
\begin{equation}
    P(X_1, X_2, ..., X_n) = \prod_{i=1}^{n} P(X_i | Pa(X_i))
\end{equation}
Here, \( Pa(X_i) \) denotes the set of parent nodes of \( X_i \) in the graph.

\subsection{Bayesian Networks}
Bayesian Networks are a specific type of graphical model that use Bayesian statistics to represent probabilistic relationships among variables. They are directed acyclic graphs (DAGs), where each edge has a direction, indicating a causal or influence relationship from one variable to another. In a Bayesian Network, the joint probability distribution is defined by the product of conditional probabilities associated with each node, given its parents in the graph.
The conditional probability of a node \( X_i \) given its parents \( Pa(X_i) \) in a Bayesian Network is typically represented by:
\begin{equation}
    P(X_i | Pa(X_i)) = \frac{P(X_i, Pa(X_i))}{P(Pa(X_i))}
\end{equation}
This equation shows that the probability of \( X_i \) depends on the joint probability of \( X_i \) and its parents, normalized by the probability of the parents. 
Bayesian Networks are used in scenarios where we want to model {\bf causal relationships} and perform {\bf reasoning under uncertainty}, e.g. medical diagnosis, genetics, machine learning, and artificial intelligence. 
One of the key tasks in working with Bayesian Networks is inference, which involves computing the {\bf posterior distribution} of certain {\bf variables} given {\bf evidence} about others.
This inference {\em is} efficient when compared to undirected graphs \cite{richardson2006markov}, or {\bf loopy belief propagation} \cite{smith2008dependency}.

\subsection{Graphical Structure of the QBN}
A graphical model is understood in terms of its {\em nodes} and its {\em edges}.
In the QBN, the nodes and edges are:
\begin{itemize}
    \item {\bf nodes} -- the nodes are the {\em propositions} (i.e. a sentence with a truth value)
    \item {\bf edges} -- {\em inferential links} between propositions; these are {\em virtual} in the sense that not every pair of nodes has features that link it together, but instead there are {\em implication patterns} that link any {\em premise} and {\em conclusion} that match the patterns
\end{itemize}

\subsection{Directed and Acyclic}
We assume that the graph is both {\em directed} and {\em acyclic}.
Each assumption contributes greatly to efficiency and simplicity, and the combination results in extreme efficiency, relative to unconstrained graphs.
\cite{smith2008dependency} explored non-acyclic (i.e. cyclic) graphs, and inference is inefficent and also not even guaranteed to converge.
\cite{richardson2006markov} presented the {\em Markov Logic Network}, which is a graphical model very similar to this, except they allow the graph to be {\em undirected}, which makes it intractable to compute the normalization term, especially by the standards of modern production.
Because we assume both directedness and simplicity, the inference is simlar to a neural network.
The difference is that much symbolic {\em bookkeeping} must be done in order to determine which nodes are related.

\subsection{Conjunctions and Disjunctions}
Any logical premise can be expressed as a {\em disjunction} (where parts are joined using {\em or}, represented as $\vee$) of {\em conjunctions} (where parts are joined using {\em and}, reprsented as $\land$).
Consider the first-order statement
\[ \forall x \in jack, \left[famous(x) \vee \left(rich(x) \wedge handsome(x)\right)\right] \rightarrow likes(jane, x)\]
Here, the premise 
\[ famous(x) \vee \left(rich(x) \wedge handsome(x)\right)\]
is a {\em disjunction} of $famous(x)$ and 
\[ rich(x) \wedge handsome(x)\]
which is a {\em conjunction} of $rich(x)$ and $handsome(x)$.

\subsection{Maximum Entropy for Disjunctions}
The probability of any given logical proposition $p$ can be cast as a {\em maximum entropy} problem of the form:
\begin{equation}
    p({\bf x}) = \frac{1}{1 + exp\left\{-\mathbf{w} \cdot \phi({\bf x})\right\}}
\end{equation}
Here ${\bf x}$ represents the {\em problem context} and {\bf w} reprsent the linear weights.
That is, currently, we do not make use of non-linear connections between nodes, because the act of splitting the graph into the nodes handles the problem of non-linear functions, and there would seem to be no benefit to using a multi-layer perceptron inside the logical factor, but we can leave the question open.
The problem context {\bf x} is created based on the 1) structure of the graph, 2) the probability values of the upstream propositions.
We discuss how the feature vector is created in more detail in \ref{s:inference}.
\subsection{Conjunctions}
What is the probability $P(rich(jack) \wedge handsome(jack))$?
If they are independent, we can simply multiply 
\[P(rich(jack))\cdot P(handsome(jack))\]
However, if they are not independent, we must take account of the graphical relationship between them.
Currently, we have only implemented the disjunction nodes, and leave conjunction nodes for future work.

\section{Inference Directly Over Semantic Roles}
\subsection{Overview}
One central innovation of this paper is to show how to {\bf inference} directly over a {\bf semantic role labeling} representation.
In this section we review the course of {\bf syntactic} and {\bf semantic} research, both {\bf rule-based} and {\bf probabilistic}.
I discuss that nearest work that this builds on is \cite{steedman2000, kwiatkowski2010, lewis2013combined, Zettlemoyer2012}, which espouses two crucial features: 1) use of {\bf probabilistic models}, and 2) use of a {\bf logically complete} language as the {\bf target language} for {\bf syntactic analysis}.
Then, I show how multiple phases involved in the computational workflow of \citep{steedman2000} can be ``compiled out'' (i.e. simplified away).
That is, I show how by simply defining a {\bf logical calculus} directly over the {\bf semantic role labeling} representation, we can save both {\bf implementation} and {\bf learning complexity}.
\subsection{Phases of Logic and Statistics in A.I. Research}
Let us review the history of {\bf natural language processing}, and see how it points in the direction of probabilistic, logical models.
\subsubsection{Rule-Based Syntax}
Modern symbolic linguistics begins with \cite{Chomsky1957SyntacticStructures}, followed by \citep{Chomsky1981GovernmentAndBinding, Chomsky1995MinimalistProgram}.
Two features characterize Chomsky's work:
\begin{enumerate}
    \item {\bf focus on syntax}: Chomsky believes that {\em syntactic structures} are relevant in and of themselves, and can and should be studied {\em without} reference to the semantics, let alone a {\em complete} logic as the endpoint.
    \item {\bf anti-probabilistic outlook}: Chomsky has always derided and continues to deride probabilistic methods, including ``machine learning'' and ``artificial intelligence,'' in the understanding of the human mind.
\end{enumerate}
By definition, zero practitioners of artificial intelligence or statistical natural language processing join Chomsky in continuing to doubt the relevance of probabilistic models to the study of human language.
Also, no one who implements a product joins Chomsky in believing is the endpoint of syntax: to build a practical system one must map a natural language sentence to its {\em semantic interpretation}.

\subsubsection{Rule-Based Logic Parsing}
Several efforts were made to create {\bf logical database representations} of {\bf world-knowledge} using {\bf manual data entry} \cite{lenat1990blk,wiki:freebase}.
\cite{pereira1987prolog} documents interesting approaches to rule-based logic programming.
The problem with hand-entered approaches to logical databases is that there are far too many facts to encode manually, no matter what one's budget, or even using crowd-sourcing.

\subsubsection{Probabilistic Syntactic Parsing}
\cite{rabiner1986maximum, rabiner1989tutorial, Jelinek1997} studied  generative speech recognition and part-of-speech tagging.
\cite{collins1999head, charniak1997statistical} studied generative phrase-structure parsing.
\cite{eisner1996bilexical} studied generative dependency parsing.
\cite{mcdonald2006online, mcdonald2005non} studied discriminative dependency parsing.
\cite{charniak-johnson-2005-coarse, huang-chiang-2007-forest} studied discriminative phrase-structure parsing.
The problem with all analyses that end in syntax is the same: we need a semantic interpretation in order to build applications.

\subsubsection{Probabilistic Semantic Parsing}
\cite{macartney2007natural,steedman2000,lewis2013combined,kwiatkowski2010,Zettlemoyer2012} studied probablistic parsing to semantic structures.
But, such {\bf semantic} models were never gotten to work directly for {\bf unsupervised} training like the LLM was.
Thus, the {\em probabilistic} models have always been limited by the same factor as the {\em rule-based} models: the ability to manually input knowledge, whether logical or statistical.
\subsubsection{Large Language Models}
LLM's are an interesting an unexpected interpolation of syntax and semantics.
Going by the $n$-gram prediction nature of the LLM, it is a {\em syntactic} model.
However, perhaps owing to the query-key-value nature of the attention mechanism, the LLM {\em is} able to acquire something that is close enough to ``world knowledge'' to work in products like {\em ChatGPT}.
Modeling world knowledge is typically considered the domain of ``semantics.''
Thus, the LLM combines elements of syntax and semantics.
However, the hallucination problem, along with our proposed diagnosis as resulting from not modeling causality, underlines the need to explicitly model semantics {\em logically}.

\subsection{The Complexity of Translation to First-Order Logic}
\subsubsection{Probabilistic Categorial Grammar as a Basis}
The overall architecture that situates the QBN is based on \cite{BarHillel1953, steedman2000,lewis2013combined,kwiatkowski2010,Zettlemoyer2012}.
This is characterized by:
\begin{enumerate}
    \item {\bf probabilistic models}: the use of {\em probabilistic models} is relevant, both for creating {\em technology} and for understanding {\em psychology}
    \item {\bf complete logical language} for semantics: the role of {\em syntax} is to map a {\em surface form} (i.e. spoken or written word) to a {\em logical form}, corresponding intuitively to its ``meaning'' or ``semantics,'' in a consistent and complete logical language
\end{enumerate}
\cite{Steedman1996,steedman2000} proposes that the role of syntax is to map surface structure to a first-order logical form.
Theoretically, this is a sound picture, because indeed the first-order logic is complete (and consistent).
However there are linear and even supra-linear complexity constants inherent in a category grammar-based translation to literally first-order logic.
\subsubsection{Complexity of Translating Into FOL}
\cite{lewis2013combined} presents the following graph of how a first-order logic formula can be created from a CCG parse:
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/lewis2013.png}
    \caption{{\em Figure 1} from \cite{lewis2013combined}. Their description included.}
    \label{fig:lewis2013}
\end{figure}
We see that this syntactic analysis maps the natural langauge sentence {\em all dogs bark} to the first-order logic formula $\forall x, dog(x) \rightarrow bark(x)$.
This is good, but the problem with the approach of getting to first-order logic by the mechanism of complex lexical categories is:
\begin{itemize}
    \item {\bf complex cateogries}: CCG categories are complex. \cite{hockenmaier2007ccgbank} derived over 1200 lexical categories from the {\em Penn Treebank} \cite{Marcus1993}.
        \begin{itemize}
            \item more categories are harder to label, when they need to be labeled by hand
            \item more categories are harder to learn automatically, if they need to be induced
            \item more categories make it harder to compute normalization factors (and other analogous calculations)
        \end{itemize}
\end{itemize}

\subsubsection{Translating Out of FOL}
The second problem with first-order logic is that we don't usually want to use a ``vanilla'' FOL formulation it directly.
What we really enjoy about using the first-order calculus is that we know that \cite{Godel1931} proved the {\em completeness} and {\em consistency} of this logic, and it is otherwise well-understood.
But, practioners typically {\em use} the first-order calculus to encode semantic knowledge in a form very much analogous to semantic role labels.

The following figure from \cite{kwiatkowski2010} shows an example of how knoweldge is written in logical systems in practice:
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{images/kwiatkowski2010.png}
    \caption{Passage taken from \cite{kwiatkowski2010}. Illustrates that, given first-order logic, we find it easier to start {\em reassembling} the semantic role structure to specify knowledge in practice.}
    \label{fig:kwiatkowski2010}
\end{figure}
In the equation
\[ \lambda x. state(x) \wedge next\_to(x, tex)\]
we are starting to recreate a tree structure in logical language using analogous symbols and struture to the directed dependency parse for the system.
That is, instead of writing {\em x borders Texas} as $next\_to(x, tex)$, we can just use the semantic-role labeling
\[ \left\{relation: next\_to, subject: x, object: tex \right\}\]
At least, we {\em could} just use this as a truth-conditional statement, if we had a proof theory and truth conditions for it.

\subsection{Compiling Out the Round Trip to FOL}
We have observed that the work of \cite{macartney2007natural,steedman2000,lewis2013combined,kwiatkowski2010,Zettlemoyer2012} implies a difficult trip from labeled dependency parse to first-order logic, in which we immediately begin expressing knowledge in a form analogous to a dependency parse.
Our proposal is that the round trip to first-order logic can be ``compiled out'' by {\em defining a logical calculus directly over the semantic role representation}.
\subsubsection{Requirements to Create a New Logic}
According to the theory \cite{Godel1931} and experience \cite{pereira1987prolog} practicing logic, we need to define a logical language and inference scheme that has the following two properties:
\begin{itemize}
    \item {\bf consistency} -- anything that is {\em provable} in the system must be necessarily {\em true}
    \item {\bf completeness} -- anything which is necessarily {\em true} must be {\em provable}
\end{itemize}
We must also add a requirement that all human knowledge must be representable in this formalism:
\begin{itemize}
    \item {\bf human capabilities} -- anything that is {\em knowable} by {\em humans} should be {\em representable} by the {\em logical language}
\end{itemize}
Empirically we know that with the {\em first-order logic} \cite{frege_begriffsschrift}, extended with {\em intensional logic} \cite{Montague1970}, can in fact represent all human language.
We will show the conditions for completeness and consistency of the {\em QBN} system, but one must remember that one forward pass through the {\em QBN} can correspond only to ``fast thinking'' \cite{Kahneman2011ThinkingFast}, and not all possible theorems require ``slow'' thinking.

\subsubsection{Inference Directly Over Semantic Role Labeling}
In order to infer directly over the semantic role representation, we just have to define a new calculus, and prove that calculus to be consistent and complete.
By staying close to the first-order logic formulation, we can inherit the proofs of \cite{Godel1931}.
We leave a detailed proof to future work.
\subsection{Difference Between Logic and Probability}
It is important to remember that first-order logic by itself, without probabilistic modeling, would {\em not} be able to model human knowledge, because human knowledge is probabilistic.
Thus, when we speak about the {\em completeness} and {\em consistency} of the logic, we mean that, in the {\em special case} where the implications of the theory are {\em rule-based} and {\em deterministic} (i.e., the implications hold 100\% of the time), we should be able to recover perfect first-order reasoning.
If probabilities are allowed, we should do whatever is rational under Bayesian assumptions.

\section{Ontology}
\subsection{Entities}
\subsubsection{Philosophical Considerations}
An entity is an atomic concept in logic.
One can get very philosophical and mystical about the question ``what is an entity.''
\subsubsection{Practical Interpretation}
However, from an information retrieval system, it is very simple: each entity is associated with a unique identifier (i.e. a string).
Associated with this unique identifier we can associate {\em properties}.
\subsubsection{Examples}
For example, in Google's {\em Knowledge Graph}, starting circa 2012, is a system which will display {\em cards} about a notable person, e.g., {\em Taylor Swift} or {\em Beyonce} is searched for.
Each one of these tracked notable figures would correspond to an {\em entity} in this information retrieval database.

\subsection{Propositions}
\subsubsection{Philosophical Considerations}
It is possible to get very philosophical about the question ``what is {\em truth}.''
As Kant intimated, and as I can now show more precisely using the mathematics of the 20th century, the concept of ``truth'' in the way most people use the word actually requires the postulation of an {\em omniscient} agent in the universe, i.e., what people would usually call ``God.''
\subsubsection{Practical Interpretation}
In practice, ropositions are statements in a {\em formal language} that can either be {\bf true} or {\bf false}.
By {\em formal language}, I mean that a clear algorithm can be given that determines whether a given {\em string} is {\em in} the language or {\em not in} the language.
\subsubsection{Examples}
In first order logic, propositions would be statements like $loves(john, mary)$, $loves(mary, john)$, $exciting(mary)$, $charming(john)$.
For reasons of implementation ease and efficiency, we do not make statements in this syntax, but using a {\em semantic role labeling}-style syntax described below.

\subsection{Predicates}
\subsubsection{Philosophical Considerations}
Once one has absorbed the metaphysical mysteries of what is an ``entity'' and what is ``truth,'' all other concepts are derviative, and so there are no more metaphysical shocks to the system.
However, we note in terms of philosophy that the concept of {\em predicate} was identified by Aristotle.
\subsubsection{Practical Interpretation}
A predicate maps some number of arguments (each of a specified {\em domain}) to a {\em truth value}.
A predicate with one argument is called {\em unary}.
A predicate with $n$ arguments is called {\em $n$-ary}.

\subsubsection{Examples}
In the first-order logic, using the {\em lambda calculus}, a predicate would be something like $\lambda x \in jill, likes(john, x)$, where $x$ is a variable.
Again, we will be using a ``semantic-role labeling'' formulation instead.

\subsection{Domain}
\subsubsection{Practical Interpretation}
A {\em domain} is a set of entities.
A domain can be seen as the set of entities that match a unary predicate.

\subsubsection{Practical Interpretation}
Based on this, the following are the two primary {\em domains} in the system right now:
\begin{itemize}
    \item {\bf entity} -- an object from a finite {\em universe}, which is finite but can always grow (bounded by resources)
    \item {\bf relation} -- corresponds to a {\em verb}, creates a {\em proposition}
\end{itemize}
Examples of {\em entity} would be things like {\em Jack} and {\em Jill}.
Examples of {\em relation} would be things like {\em likes} and {\em dates}.

\subsubsection{Examples}
The two major categories in language are the {\em entity} and the {\em relation}.
In the notation of \cite{steedman2000}, there is category {\em NP} (for noun-phrases) and {\em S} (for sentences).
This corresponds to the noted ontological distinction between {\em entities} (which are things from an unbounded universe) and {\em propositions} (which either evaluate to true or false).



\section{Semantic Role Labeling Calculus}
\subsection{Semantic Roles}
\subsubsection{Necessity of Role Labeling}
Semantic roles as they are traditionally named are things like {\em subject}, {\em object}, {\em indirect object}, etc.
In the sentence {\em John loves Sally}, {\em John} is the {\em subject} and {\em Sally} is the {\em object}.
This means that {\em John} has affections for {\em Sally}, but we don't necessarily know how {\em Sally} feels about {\em John}.
If the roles were reversed, and {\em Sally loves John}, then we would know what {\em Sally} thinks, but not {\em John}.
This is why a purely {\em unlabeled} dependency parse is not useful.

\subsubsection{Assuming a Finite Number of Role Labels}
We assume that there are a {\em finite} number of semantic roles.
This might be because we are literally {\em programmed} with a specific finite set of roles in the style of a Chomskyan {\em Universal Grammar}.
Or, it might be that an expectational maximization algorithm is {\em allowed to grow} the number of roles until convergence, at which point the number of roles will not grow any more.

\subsection{Arguments}
\subsubsection{Definitions}
Each semantic role contains as its argument either:
\begin{enumerate}
    \item a \emph{constant}, which identifies a concrete element in some universe (or database)
    \item a \emph{variable}, which ranges over some \emph{domain} of entities in the database
\end{enumerate}
A {\em constant} corresponds to $jack$ in the first-order sentence $lonely(jack)$.
A {\em variable} corresponds to $x$ in the first-order lambda-calculus predicate $\lambda x \in jack, lonely(x)$.
A variable specifies a \emph{domain}, each domain being a subset of the entities.

\subsection{Domain of Quantification for Variables}
It is important to note that any domain quantified over requires an assumption of membership.
For example, the only thing we can say about a (hypothetical) entity, like \( jack1 \), is that it is an entity, rather than a relation or a predicate.
To say that \( jack \) is a \emph{man}, \emph{woman}, \emph{human}, etc., is to say something empirical that could, \emph{technically} be true.
However, it may be that it is more efficient or otherwise beneficial for engineering reasons to strictly assume category membership.
This is what we do in our experiments, where we explicitly denote the set of \( Jacks \) versus the set of \( Jills \) to model a binary dating scenario.

In other words, consider the statement:
\[ \forall x \in Jack, y \in Jill, likes(x, y) \rightarrow dates(x, y) \]
This is equivalent to:
\[\forall x \in  y, Jack(x) \wedge Jill(y) \wedge likes(x, y) \rightarrow dates(x, y) \]
Thus, the probabilities $P(Jack(x))$ and $P(Jill(y))$ are implicitly part of the model.
The only way we can state that quantification ranges over a subset of entities in the basic type is if we are willing to model the probability of membership as definitely $1$.

\subsection{Propositions}
\subsubsection{Practical Interpretation}
Propositions are statements in the logical language that have no variables. In other words, propositions have only constants.
Because there are no variables, and every constant resolves conceptually to an {\em entity}, a {\em relation} and any other relevant type, a proposition can be assigned a {\em truth value}.
In practice, {\em prediction} and {\em compression} are more relevant words to use than {\em truth} because, as finite beings, we do not have access to the ``real truth.''
But, even from the perspective of {\em prediction} and {\em compression}, a proposition must either be 1) {\em true}, 2) false, or 3) some state in between, with probability mass distributed between the two options {\em true} and {\em false} to add up to 1.

\subsubsection{Examples}
\subsubsection{Atomic Proposition}
Using this semantic-role notation we represent the traditional first order proposition $likes(jack1, jill1)$ as:
\begin{itemize}
    \item $\left\{relation:likes, subject:jack1, object:jill1 \right\}$
\end{itemize}
Again, the probability that {\em jack1 likes jill1} be inversely related to the probability that {\em jack1 does not like jill1}.

\subsubsection{Complex Propositions}
We can conjoin sentences like the following, expressing that $jack1$ and $jill1$ both like each other:
\begin{itemize}
    \item $\left\{relation:likes, subject:jack1, object:jill1 \right\} \wedge \left\{relation:likes, subject:jill1, object:jack1 \right\}$
\end{itemize}

\subsection{Predicates}
A predicate corresponds to a first-order logic with lambda calculus express $\lambda x \in jack, lonely(x)$, which maps an element $x \in jack$ to a truth value.
\subsubsection{Predicate Over One Variable}
The simplest predicates have only one variable. These correspond to {\em sets} of entities. Aristotle called them {\em categories}.
Using the first-order lambda calculus, some unary predicates are:
\begin{itemize}
    \item $\lambda x, lonely(x)$
    \item $\lambda x, exciting(x)$
    \item $\lambda x, is_jack(x)$
    \item $\lambda x, is_jill(x)$
\end{itemize}
In the semantc-role labeling notation, this would be written as:
\begin{itemize}
    \item $\lambda x, \left\{verb: lonely, subject: x\right\}$
    \item $\lambda x, \left\{verb: exciting, subject: x\right\}$
    \item $\lambda x, \left\{category: jack, subject: x\right\}$
    \item $\lambda x, \left\{category: jack, subject: x\right\}$
\end{itemize}

\subsubsection{Predicates Over Multiple Variables}
Predicates over multiple variables are often called {\em relations}.
While a unary predicate demarcates a {\em subet} of its containing domain, a $n$-ary predicate demarcates a subset of the {\em Cartesian Product} of all of the relevant domains.
Using the first-order lambda calculus, some $n$-ary predicates are:
\begin{itemize}
    \item $\lambda x \lambda y, likes(x, y)$
    \item $\lambda x \lambda y, dates(x, y)$
\end{itemize}
In the semantc-role labeling notation, this would be written as:
\begin{itemize}
    \item $\lambda x, \lambda y, \left\{verb: likes, subject: x, object: y\right\}$
    \item $\lambda x, \lambda y, \left\{verb: dates, subject: x, object: y\right\}$
\end{itemize}
\section{Knowledge Transfer from LLM to QBN}
\subsection{Review of Assumptions}
We have advanced the hypothesis that:
\begin{description}
    \item[\textbf{LLM's Have World Knowledge}] We are assuming that LLM's {\em do} have world knowledge. This could be because the query-key-value function of the attention mechanism is actually discovering the semantic role nature of natural language.
  \end{description}

And, at this point we are assuming that the QBN is an ``appropriate'' knowledge engine:
\begin{description}
    \item[\textbf{QBN is ``Appropriate''}] By ``appropriate'', I mean that whatever requirements we have of an {\em inference engine}, an ``appropriate'' engine meets all of those.
  \end{description}

So, with these two assumptions, then since 1) knowledge is in the LLM and 2) if we could get it into the QBN we would be good, the question now becomes: how do we transfer knoweldge from the LLM to the QBN?

\subsection{Latent Syntactic Structure}
\subsubsection{CFG Parsing}
The original annotation standard was CFG phrase-structure labeling.
This labeling scheme recursively divids a sentence into recursively contained, non-overlapping {\em sub-spans} of the sentence.
This is like the CFG phrase-structure described by Chomsky.
However, the phrase-sturcture formalism was perhaps always just a compromise: instead of picking some complex, theory-laded formalism on offer from Linguistics, the phase-structure representation is the simplest phrase-structure syntactic analysis possible.
Thus, while phrase-structure parses deeply offend no one, they also endear themselves to no one: working with a CFG parse is not convenient or powerful.
\subsubsection{Unlabled Dependency Parsing}
One way to look at a phrase-structure parse is not in terms of {\em spans}, but in terms of lexical head-modifier {\em dependencies}.
This is attractive because the word-word dependencies seem more semantically ``meaningful'' than phrase structures.
In other words, in {\em John loves Mary}, intuitively the relationship between {\em John} and {\em loves}, or between {\em Mary} and {\em John} is, intuitively, more relevant than the indicies of the span that either phrase takes up.
A flurry of work looked at learning dependency parisng directly.
\subsubsection{Labled Dependency Parsing}
However, in practice, it is virtually useless to have a sentence that is annotated with only an unlabeled parse.
In order to use a parse, one must not only know {\em which words modify which heads}, but also to know the {\em label} sub-type of that modification.
For example, in {\em John loves Mary}, both {\em John} and {\em Mary} modify the verb {\em love}, however knowing who loves who requires knowing which is the {\em subject} and which is the {\em object}.
Reversing the roles played by the arguments reverses the meaning of the sentence.
Fortunately, algorithms that work well for unlabeled dependency parsing also work well for labeled dependency parsing.

\subsubsection{Parsing to Semantics}
Bar-Hillel showed how a semantic intepretation can be created for a surface form as a by-product of parsing using a {\em categorial grammar}.
Steedman's {\em combinatory categorial grammar} showed how to extend this to cases with non-projective dependencies.
Ultimately, we can create a semantic using semantic role labeling, any time we can get a {\em labeled dependency parse}.
The problem with traditional ``labeled dependency parsing'' is that these labels are automatically extracted from a CFG-labeled treebank (e.g. Penn Treebank).
Thus, the labels in these schemes do not really correspond to any actual logical inference language that can be used for thinking.
So, the task in practice is to arrive at a {\em labeled} dependency parse.
The question of which {\em labels} exist, and how we will determine them are open questions.
Assuming that there {\em is} some labeling, we can begin discussing how to learn it.

\subsection{Generative Syntactic Models}
Now, there are generative models, especially the LLM.
Before this trend, the most accurate syntactic analysis methods were {\em discriminative}, meaning that they could estimate the {\em conditional} probability of an analysis given a sequence of tokens, but could not generate the tokens based on a generative model.

\subsection{A Generative Transformer-Based Model}
In order to generate the surface form (words data), there will need to be a {\em generative} model that generates {\em labeled dependency parses}.
The parses are latent so must be estimated with {\em expectation maximization} \cite{baum1966statistical,Jelinek1997, rabiner1989tutorial}.
The generative probability of generating a parse is a function of 1) the logical probability according to the QBN that a given logical form would be expressed in a given linguistic context, 2) the syntactic probability that a given logical form would be expressed in a particular way.

\subsection{Syntactic Parses are Latent States}
The problem with using syntactic parses, relative to LLM's--and this is probably the reason n-gram LLM's worked before structured ones--is that one must {\em infer} a latent state.
That is, the {\em correct} syntactic analysis is ``hidden.''
This corresponds to the fact that we can {\em definitely} see what a person has written, or hear what they have said.
But, we can {\em never} be sure that we know exactly what they {\em meant}.
This is the basis for someone saying that they have been ``taken out of context.''
In other words, the allegation that one is being taken out of context is the allegation that their {\em intended} latent meaning is not the one being ascribed.
Thus, clearly, even humans cannot be sure about the {\em correct} analysis, if by ``correct'' we mean ``intended.''


\section{Future Work}
\subsection{Transferring LLM Knowledge to QBN}
In order to build full AGI, we need to combine the LLM, which has the knowledge now, to the QBN, which can store the data in a ``logically perfect'' way.
Although this is an area where litle is known, it is an area where we believe it is easy to work along the obvious dimensions.

\subsection{More Complex Logical Problems}
We would probably actually be more interested in developing a way to encode complex logical problems.

\subsection{Functions (Beyond Predicates)}
Another question is the extension to functions.
The ability for human people to do math and calculate sums and products is an interesting phenomenon.
To what extent is addition an example of {\em thinking fast}?
Well, obviousy people must use paper for complicated operations, so it must be thinking slow.
But, what about the most basic operations?
Perhaps we are equipped to compute basic calculations like $2 * 2$ fast, but $22 * 22$ requires paper or a good memory to manage those two operations.
In any event, it would seem like the most efficient option for building technology would {\em not} be to mimic the human mind exactly, since computers are better at computing.
Instead, we would probably want to break with whatever humans do for complex calculations, and instead just use the computer.
Karpathy TODO has said that in {\em OpenAI}'s strategy is to use hard-coded functions for things like math, and only use the LLM to detect when a math problem is relevant.

\subsection{Distinguish Between {\em AND} and {\em OR} Junctions}
The maximum entropy junction represents the {\em OR}-junction in the boolean network.
Future work must support {\em AND}-junctions as well.

\subsection{More Efficient Implementation}
Currently, the implementation is in {\em NodeJS} but we expect switching to {\em Rust} and using an optimized {\em string serialization} will speed up all operations.
Also, we must collect more speed data.

\subsection{A Theory of Creativity}
We believe that the QBN leads to a theory of creativity.
Creativity is whenever new connections are tried in the network.
We save a more fine-grained analysis of creativity for future work.
\subsection{Alignment}
The problem of aligning the actions of artificial intelligence to the values of humans is one of the most important topics at the intersection of philosophy and computer science today.
The symbolic approach to artificial intelligence gives us an avenue for alignment because we can interpet a symbolic theory, and plan to force the artificial intelligence to respect values that we give it.
\section{Conclusion}
We have presented the Quanified Bayesian Network, a probabilistic graphical model that allows for 1) a generative model of the logical forms behind language, 2) that can explain its reasoning, and so not hallucinate
We have described how the QBN must be paired with a tree-structured generative model that generates the surface form (words) data from the logical form chosen.

\bibliographystyle{apalike}
\bibliography{bibtex}

\end{document}


