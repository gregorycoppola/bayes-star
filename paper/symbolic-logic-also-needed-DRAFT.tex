\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{hyperref}
\title{Symbolic Logic is Also Needed}
\author{Greg Coppola}
\date{\today}

\begin{document}
\maketitle


\begin{abstract}
In this paper, we present the {\bf Quantified Bayesian Network} ({\bf QBN}).
This is a data structure that, we believe, can end the problem of {\em LLM hallucination}.
The {\bf QBN} provides a {\bf generative model} of the {\bf logical structure} behind the sentences in natural language.
If the world knowledge of the LLM can be transferred into the QBN, we believe that that will constitute AGI.
The QBN is able to generative model data.
It is a generalization of both first-order predicate logic, as well as the Bayesian Network.
A QBN can represent logical relationships between logical propositions.
\end{abstract}
\section{Introduction}
The {\bf Large Language Model} ({\bf LLM}) is the hottest data structure in artificial intelligence and computer science right now.
Pioneered by the product {\em ChatGPT} by {\em OpenAI},
leading to a robust competition in artificially intelligent agents, often called ``chat bots,'' or simply {\em LLM}'s.
There is widespread agreement that the {\em LLM} is a powerful technology.
For example, neutral but well-known investor Jason Calacanis recently said of the {\em LLM}-based technology boom:
\begin{quote}
    AI is the opportunity of our lifetimes... 
    bigger than the internet, PCs, mobile, and cloud -- combined.  
    \cite{Jason2023}
\end{quote}
But, there are some major, well-known limitations with {\em LLM}'s that limit their applicability.
The main problem is {\bf hallucinations}: the LLM does not know what it does not know, and this lack of reliability limits the ability of the system to be used fully automatically, i.e., without a human ``in the loop.''
\section{Limitations of LLM's}
\subsection{Hallucinations}
The model is ``hallucinating'' when it is answering a question with an answer completely unsuported by the training data.
This is the most commonly listed problem with LLM's.
Sutskever \cite{sutskever:huang:2023} has said this limits the usefulness of LLM's because there must always be a human ``in the loop'', and so things cannot be fully automated.
\subsection{Cannot Do Reasoning}
It is known that LLM's cannot do reasoning.
Hinton \cite{hinton:cbs:2023} has underlined the problem that LLM's 1) do not have a ``logically consistent worldview'' and 2) do not understand how different ``worldviews'' (what we call {\em theories}) lead to different conclusions.
\subsection{Our Analysis}
These two problems are related.
In order to not hallucinate, a model must be able to explain {\em why} it believes what it does.
But the ability to answer {\em why} implies an understanding of {\em causality}.
And, {\em causality} is just another way of saying {\em logical inference}, because you cannot have one without the other.
\section{Existing Approaches to Hallucinations}
\subsection{Fine-Tuning}
Fine-tuning is the strategy that {\em ChatGPT} originally used to constrain the generative model\cite{radford2018improving,radford2019language,brown2020language},
and is also used in the competing products.
Fine-tuning is a process by which access to the activation outputs of the underlying LLM generative model are used
to train a {\em discriminative} model to achieve certain tasks.
This gives an option to reduce many kinds of LLM erros, but not all, because hallucination remains an unsolved problem.
Also, we think we should aesthetically object to the {\em necessity} of a discriminative model in order to not hallucinate.
If the spirit of the LLM is to be {\em generative}, we should push for a generative model that can avoid hallucinations on its own.
\subsection{Retrieval Augmented Generation}
{\bf Retrieval Augmented Generation} (RAF) is a process by which access to an extrinsic traditional {\em discrete} database-based information
product, especially a {\em search engine} \cite{lewis:2020}.
The idea is that a discriminative or even programmatic process can be created that 1) uses the LLM activations as features, and 2) has access to the discrete outputs of the traditional search engine.
This way, the content of the answer can be checked against the web page.
One limitation of this is that {\em it does not let the LLM make its own theory}.
We are limited to agreeing with the web page.
Also, as with fine-tuning, we believe it is an aesthetic problem that the {\em generative} part of the model cannot stop hallucinating by itself.
RAG can also be referred to as {\em trusted sources}.
\subsection{Vector Databases}
Vector databases have been proposed as a solution to hallucinations.
However, vectors do not have any relation to logical calculus that is established.
Thus, there is no concept of {\em true} or {\em false} with vectors in a vector database any more than there is truth or falsity in the LLM.
Also, empirically, this approach has not been gotten to work, despite being suggested as early as {\em ChatGPT}'s hallucination issue was realized.
\section{Logic}
Logic comes in a a few forms, varying in their levels of complexity, and scope.

\subsection{Propositional Logic}
The most basic logic is {\em propositional logic}.
Propositional logic deals with statements like $A$, $\neg A$, $A \wedge B$, $A \rightarrow B$.
This is limited because it does not allow different propositions to ``share'' any of their ``meanings''.
That is, in the propositional calculus, if $A$ represents the sentence {\em John runs} and $B$ represents
the sentence {\em Mary runs}, there is no way to represents that both sentences share something
in common, because $A$ and $B$ are different letters, with different indices, sharing no reusable parts.

\subsection{First-Order Logic}
\subsubsection{Compositional Structure}
First-order logic solves this problem of sharing structure between similar sentences by
giving us {\em predicates}, {\em entities} and {\em quantifiers}.
Thus, we can say that {\em John runs} as $run(john)$ and {\em Mary runs} as $run(mary)$,
and we see that both expression share some part of the structure, $run$, corresponding
to our idea that the surface forms {\em John runs} and {\em Mary runs} are related.
\subsubsection{Quantification}
First-Order Logic also makes use of {\em quantifiers}.
The sentence $\forall x, late(x)\rightarrow runs(x)$ says that {\em for any person, if they are late, they are running}.
This kind of sentence allows us to identify {\em patterns} between facts, e.g., being late and running.
The problem with First-Order Logic is that it is too {\em deterministic}.
In real-life scenarios, we exploit patterns that are true {\em most} but not {\em all} of the time.
Thus, it might be that being late {\em tends to lead} to running, but it doesn't always.

\subsection{Beyond First-Order Logic}
First-order logic is not enough to model all human natural language.
This is not to say that it should be thrown out, but that there are certain ``extensions'' to first-order logic that are needed to model the most sophisticated examples of natural language.
\subsubsection{Intensional Logic}
First of all, {\em intensional logic} is needed in order to allow the ``concept'' of a sentence to be the argument to a predicate, rather than simply its truth value (one of {\em true} or {\em false}).
Intensional logics are less well-agreed upon that first-order logic.
But, in any case, they use the same mechanism of universal quantification, only over a specially constructed entity set, based on the logical language.
\subsubsection{Compositional Semantics}
Second, first-order logic by itself does not tell us how to build up the meanings of complicated expressions, e.g. {\em the Mayor of New York}, {\em the Mayor of New Delhi}, from their simpler parts.
This is again simply an area where there is less clarity than first-order logic, only because there are more options, but there are many options to choose from.
\subsection{Conclusion}
In the following work, we will focus on the first-order logic.
First, because all other logics are extensions of the first-order logic, so we can reuse the same techniques for training a first-order system.
Second, for simplicity.
Third, because there is not a consensus on the details of the more complicated extended logics.
\section{Bayesian Networks}
We want to look at a certain kind of data structure in computer science: the Bayesian Network.
\subsection{Basic Concepts}

A Bayesian Network, \textbf{B}, consists of:

\begin{itemize}
    \item A set of variables $\textbf{X} = \{X_1, X_2, \ldots, X_n\}$.
    \item A set of directed edges \textbf{E} between these variables, which represent causal relationships.
\end{itemize}

Each variable \(X_i\) is associated with a probability distribution that quantifies the effects of the parents of \(X_i\) in the graph.

\subsection{Advantages of Bayesian Networks}

Bayesian Networks offer several advantages:

\begin{itemize}
    \item They provide a clear and visual representation of causal relationships.
    \item They allow for efficient computation of conditional probabilities.
    \item They enable the integration of both prior knowledge and observed data.
\end{itemize}
\section{Ontology}
\subsection{Entities}
\subsubsection{Philosophical Considerations}
An entity is an atomic concept in logic.
One can get very philosophical and mystical about the question ``what is an entity.''
\subsubsection{Practical Interpretation}
However, from an information retrieval system, it is very simple: each entity is associated with a unique identifier (i.e. a string).
Associated with this unique identifier we can associate {\em properties}.
\subsubsection{Examples}
For example, in Google's {\em Knowledge Graph}, starting circa 2012, is a system which will display {\em cards} about a notable person, e.g., {\em Taylor Swift} or {\em Beyonce} is searched for.
Each one of these tracked notable figures would correspond to an {\em entity} in this information retrieval database.

\subsection{Propositions}
\subsubsection{Philosophical Considerations}
It is possible to get very philosophical about the question ``what is {\em truth}.''
As Kant intimated, and as I can now show more precisely using the mathematics of the 20th century, the concept of ``truth'' in the way most people use the word actually requires the postulation of an {\em omniscient} agent in the universe, i.e., what people would usually call ``God.''
\subsubsection{Practical Interpretation}
In practice, ropositions are statements in a {\em formal language} that can either be {\bf true} or {\bf false}.
By {\em formal language}, I mean that a clear algorithm can be given that determines whether a given {\em string} is {\em in} the language or {\em not in} the language.
\subsubsection{Examples}
In first order logic, propositions would be statements like $loves(john, mary)$, $loves(mary, john)$, $exciting(mary)$, $charming(john)$.
For reasons of implementation ease and efficiency, we do not make statements in this syntax, but using a {\em semantic role labeling}-style syntax described below.
\subsection{Predicates}
\subsubsection{Philosophical Considerations}
Once one has absorbed the metaphysical mysteries of what is an ``entity'' and what is ``truth,'' all other concepts are derviative, and so there are no more metaphysical shocks to the system.
However, we note in terms of philosophy that the concept of {\em predicate} was identified by Aristotle.
\subsubsection{Practical Interpretation}
A predicate maps some number of arguments (each of a specified {\em domain}) to a {\em truth value}.
\subsubsection{Examples}
In the first-order logic, using the {\em lambda calculus}, a predicate would be something like $\lambda x \in jill, likes(john, x)$, where $x$ is a variable.
Again, we will be using a ``semantic-role labeling'' formulation instead.

\section{Semantic Role Labeling Formulation}
\subsection{Semantic Roles}
\subsubsection{Necessity of Role Labeling}
Semantic roles as they are traditionally named are things like {\em subject}, {\em object}, {\em indirect object}, etc.
In the sentence {\em John loves Sally}, {\em John} is the {\em subject} and {\em Sally} is the {\em object}.
This means that {\em John} has affections for {\em Sally}, but we don't necessarily know how {\em Sally} feels about {\em John}.
If the roles were reversed, and {\em Sally loves John}, then we would know what {\em Sally} thinks, but not {\em John}.
This is why a purely {\em unlabeled} dependency parse is not useful.

\subsubsection{Assuming a Finite Number of Role Labels}
We assume that there are a {\em finite} number of semantic roles.
This might be because we are literally {\em programmed} with a specific finite set of roles in the style of a Chomskyan {\em Universal Grammar}.
Or, it might be that an expectational maximization algorithm is {\em allowed to grow} the number of roles until convergence, at which point the number of roles will not grow any more.

\subsection{Arguments}
Each semantic role contains as its argument either:
\begin{enumerate}
    \item a \emph{constant}, which identifies a concrete element in some universe (or database)
    \item a \emph{variable}, which ranges over some \emph{domain} of entities in the database
\end{enumerate}

A variable specifies a \emph{domain}, each domain being a subset of the entities.

It is important to note that the entities are very high-level type categories, constant, finite and fixed in number ahead of any data.
We choose the domains \emph{entity}, \emph{relation} and \emph{predicate}.
Entities refer to ontological \emph{objects}, like \emph{jack1} and \emph{jill1}.
Relations refer to what we would call \emph{verbs} in linguistic analysis, such as \emph{likes} or \emph{dates}.
Predicates, in this formulation, are unary predicate words like \emph{lonely} or \emph{exciting}.

It is important to note that any domain quantified over requires an assumption of membership.
For example, the only thing we can say about a (hypothetical) entity, like \( jack1 \), is that it is an entity, rather than a relation or a predicate.
To say that \( jack \) is a \emph{man}, \emph{woman}, \emph{human}, etc., is to say something empirical that could, \emph{technically} be true.
However, it may be that it is more efficient or otherwise beneficial for engineering reasons to strictly assume category membership.
This is what we do in our experiments, where we explicitly denote the set of \( Jacks \) versus the set of \( Jills \) to model a binary dating scenario.

In other words, consider the statement:
* $\forall x \in Jack, y \in Jill, likes(x, y) \rightarrow dates(x, y)$
This is equivalent to:
* $\forall x \in  y, Jack(x) \wedge Jill(y) \wedge likes(x, y) \rightarrow dates(x, y)$

Thus, the probabilities $P(Jack(x))$ and $P(Jill(y))$ are implicitly part of the model.
The only way we can state that quantification ranges over a subset of entities in the basic type is if we are willing to model the probability of membership as definitely $1$.

\subsection{Propositions}
Propositions are statements in the logical language without quantification.

Using this semantic-role notation we represent the traditional first order proposition $likes(jack1, jill1)$ as:
\begin{itemize}
    \item $\left\{relation:likes, subject:jack1, object:jill1 \right\}$
\end{itemize}

We can conjoin sentences like the following, expressing that $jack1$ and $jill1$ both like each other:
\begin{itemize}
    \item $\left\{relation:likes, subject:jack1, object:jill1 \right\} \wedge \left\{relation:likes, subject:jill1, object:jack1 \right\}$
\end{itemize}

\subsection{Predicates}
In traditional first-order logic, a predicate would be a statement like $likes(x, y)$ in the overall sentence $\forall x\in Jack, \forall y \in Jill, likes(x,y) \rightarrow dates(x, y)$.

We will refer to any statement in which the arguments contain at least one variable.

In other words, in our logical language, predicates include functions like:
\begin{itemize}
\item $\left\{relation:likes, subject:{x \in Jack}, object:{y \in Jill} \right\}$
\item $\left\{predicate:lonely, subject:{x \in Jack} \right\}$
\item $\left\{predicate:exciting, subject:{x \in Jill} \right\}$
\end{itemize}

In other words, a predicate is intuitively a function, where the input types are specified in the predicate defintion, and the output is a binary value.

\section{Implementing Implication}
\subsection{Predicate Matcher}
From one perspective, a predicate is a mapping from inputs to truth values, as we saw.
From another perspective, we can view a predicate as being mapped one-to-one with a *proposition matcher*.
The proposition matcher associated with a predicate $q$ matches any proposition $p$ such that $p$ is an instantiation of $q$, with values of the specified types.

For example, 
$p = \left\{relation:likes, subject:jack1, object:jill1 \right\}$
is an instantiation of
$q = \left\{relation:likes, subject:{x \in Jack}, object:{y \in Jill} \right\}$
using the subsitution $x=jack1$ and $y=jill1$.

Thus, $match(q, p)$ is true.
However, $p$ does *not* match $q2$, because its values for relation is $dates$, which does not equal $likes$.
$q2 = \left\{relation:dates, subject:{x \in Jack}, object:{y \in Jill} \right\}$
using the subsitution $x=jack1$ and $y=jill1$.

Similarly, $p$ does not match $q3$, because $q3$ specifies that the subject must be a $Jill$, not a $Jack$.
$q2 = \left\{relation:dates, subject:{x \in Jill}, object:{y \in Jack} \right\}$

\subsection{Substitution}
\subsection{Implication Links}
The overall concept of the Quantified Bayesian Network is to allow a trainable network encoding and generalizing the concept of first-order quantification.
That is, we want to generalize the ability to make statements like $\forall x, y exciting(y) \rightarrow likes(x, y)$.
However, we must emphasize that the *entire point* of training a machine learned system is that statements of the form ``*all* X are Y'' rarely hold in practice.
But, advantage is gained merely by noticing a correlation.

So, what then is first-order implication if we relax the literal logical implication?
From our perspective, the generalization is simply to say that the premise and conclusion of first-order logic are *related* in some way (or, at least possibly related), and it is up to the model training to decide which.

\subsection{Matcher}
A matcher takes a predicate \( q \) and a proposition \( p \) and returns \( \text{match}(p, q) \):
\begin{enumerate}
    \item None -- if \( p \) does not match \( q \)
    \item Some(\( r \)) -- where \( r \) is a role mapping \( \{r: a\} \) such that \( \text{substitute}(q, r) = p \)
\end{enumerate}

\subsection{Substitution Rule}
A substitution rule takes in a predicate \( q \) and a role mapping \( r \) and gives back a proposition \( p \), by:
\begin{enumerate}
    \item type-checking that the role map \( r \) corresponds to the arguments in \( p \)
    \item substituting the roles in role map \( r \) as the variables to premise predicate \( q \)
\end{enumerate}

For example, the value of:
\[ \text{substitute}(\{\text{relation}: \text{predicate}, \text{subject}:\{x \in \text{Jack}\}\}, \{\text{subject}:\text{jack1}\}) \]
is \( \{\text{relation}: \text{predicate}, \text{subject}:\text{jack1}\} \).



\subsection{Forwards Links and Backwards Links}
The generative story proceeds ``forwards''.
That is, where the nodes of the Quantified Bayesian Network form a Directed Acyclic Graph, we recursively can compute the value of a node \( n \) by doing a depth-first traversal of its ancestors in the DAG.
We refer to the link from a premise to a conclusion as a \emph{forward} link.

Thus, in order to find the possible causes of a given node, we must traverse the edge in the opposite direction.
We call the link from a conclusion to a premise a backwards link.

We will focus on forwards inference in this paper, and leave backwards inference for future work.
Thus, we are primarily concerned with \emph{backwards} links, and this is what is currently implemented in the BAYES STAR package.

\section{Generalization Results}
We propose that the {\bf QBN} is a generalization of both:
\begin{enumerate}
    \item {\em fast thinking} -- this refers to ``forward'' thinking, with a fixed, trained QBN
    \item {\em slow thinking} -- this refers to the ability to reason with {\em or} conclusions
    \item {\em creative thinking} -- this refers to the creation and training of {\em new links}
\end{enumerate}

We believe it is correct in a ``strong'' sense to say that:
\begin{itemize}
    \item the Quantified Bayesian Network {\em generalize} some {\em important faction} of first-order logic
\end{itemize}

Now, here we require some care because we do {\em not} believe that a practical computational system {\em should} generalize full first-order logic in an overly specific way.
This is because there are different kind sof thinking.

\subsection{Thinking Fast And Slow}
\subsubsection{The Literature}
{\em Thinking Fast and Slow} is a book by TODO that popularized the idea that there are ``fast'' and ``slow'' kinds of thinking.
Let us put it this way.
Everyone can speak and locomocate around the world.
An act as simple as crossing the street requires some kind of logical calculation, based on the goal of {\em not getting hit by a car}.

Thus, we see that all people can think.


But, not everyone can do quantum physics and not everyone can do complex mathematics.
Major mathematical results often take years of collective work by scientists.
{\em Fermat's Last Theorem} took TODO years to solve.
Obviously some kinds of thinking are slower than others!

\subsubsection{Our Interpretation}
We propose that there are three kinds of {\em speeds} of {\em thinking}:
\begin{enumerate}
    \item {\em fast thinking} -- this refers to ``forward'' thinking, with a fixed, trained QBN
    \item {\em slow thinking} -- this refers to the ability to reason with {\em or} conclusions
    \item {\em creative thinking} -- this refers to the creation and training of {\em new links}
\end{enumerate}

\subsection{Fast Thinking in a QBN}
We will use the analogy from psychology to describe the QBN.
This is not surprising, because the QBN is modeled on the human brain.

We believe that the {\em fast thinking} subset of first-order logic is that part which:
\begin{enumerate}
    \item only uses conclusions that do not contain the symbol $\cup$ TODO
\end{enumerate}

That is, let us look at some {\em complex} and {\em simple} statements:
\begin{enumerate}
    \item if A and B, then C -- simple
    \item if A or B, then C -- simple
    \item if A, then B and C -- simple
    \item if A, then B or C -- complex
\end{enumerate}

Intuitively, the complex situation is when we learn that either one thing is true {\em or another}, and we don't know which.

For example, consider the planning of an outting to the beach.
If we get reliable information that it {\em will} rain, then we can pick an indoor activity instead.
If we get reliable information that it {\em will not} rain, then we can plan the trip to the beach.
If we don't know if there will be rain, we are in a complicated position.
Now, we must come up with a strategy that works for both cases (rain and not rain).
A full decision analysis would require us to:
\begin{enumerate}
    \item determine the probability of rain
    \item determine possible courses of action
    \item define the utility of each possible outcome
    \item compute the expected utility
\end{enumerate}
\section{Knowledge Transfer from LLM to QBN}
\subsection{Review of Assumptions}
We have advanced the hypothesis that:
\begin{description}
    \item[\textbf{LLM's Have World Knowledge}] We are assuming that LLM's {\em do} have world knowledge. This could be because the query-key-value function of the attention mechanism is actually discovering the semantic role nature of natural language.
  \end{description}

And, at this point we are assuming that the QBN is an ``appropriate'' knowledge engine:
\begin{description}
    \item[\textbf{QBN is ``Appropriate''}] By ``appropriate'', I mean that whatever requirements we have of an {\em inference engine}, an ``appropriate'' engine meets all of those.
  \end{description}

So, with these two assumptions, then since 1) knowledge is in the LLM and 2) if we could get it into the QBN we would be good, the question now becomes: how do we transfer knoweldge from the LLM to the QBN?

\subsection{Latent Syntactic Structure}
\subsubsection{CFG Parsing}
The original annotation standard was CFG phrase-structure labeling.
This labeling scheme recursively divids a sentence into recursively contained, non-overlapping {\em sub-spans} of the sentence.
This is like the CFG phrase-structure described by Chomsky.
However, the phrase-sturcture formalism was perhaps always just a compromise: instead of picking some complex, theory-laded formalism on offer from Linguistics, the phase-structure representation is the simplest phrase-structure syntactic analysis possible.
Thus, while phrase-structure parses deeply offend no one, they also endear themselves to no one: working with a CFG parse is not convenient or powerful.
\subsubsection{Unlabled Dependency Parsing}
One way to look at a phrase-structure parse is not in terms of {\em spans}, but in terms of lexical head-modifier {\em dependencies}.
This is attractive because the word-word dependencies seem more semantically ``meaningful'' than phrase structures.
In other words, in {\em John loves Mary}, intuitively the relationship between {\em John} and {\em loves}, or between {\em Mary} and {\em John} is, intuitively, more relevant than the indicies of the span that either phrase takes up.
A flurry of work looked at learning dependency parisng directly.
\subsubsection{Labled Dependency Parsing}
However, in practice, it is virtually useless to have a sentence that is annotated with only an unlabeled parse.
In order to use a parse, one must not only know {\em which words modify which heads}, but also to know the {\em label} sub-type of that modification.
For example, in {\em John loves Mary}, both {\em John} and {\em Mary} modify the verb {\em love}, however knowing who loves who requires knowing which is the {\em subject} and which is the {\em object}.
Reversing the roles played by the arguments reverses the meaning of the sentence.
Fortunately, algorithms that work well for unlabeled dependency parsing also work well for labeled dependency parsing.

\subsubsection{Parsing to Semantics}
Bar-Hillel showed how a semantic intepretation can be created for a surface form as a by-product of parsing using a {\em categorial grammar}.
Steedman's {\em combinatory categorial grammar} showed how to extend this to cases with non-projective dependencies.
Ultimately, we can create a semantic using semantic role labeling, any time we can get a {\em labeled dependency parse}.
The problem with traditional ``labeled dependency parsing'' is that these labels are automatically extracted from a CFG-labeled treebank (e.g. Penn Treebank).
Thus, the labels in these schemes do not really correspond to any actual logical inference language that can be used for thinking.
So, the task in practice is to arrive at a {\em labeled} dependency parse.
The question of which {\em labels} exist, and how we will determine them are open questions.
Assuming that there {\em is} some labeling, we can begin discussing how to learn it.

\subsection{Generative Syntactic Models}
Now, there are generative models, especially the LLM.
Before this trend, the most accurate syntactic analysis methods were {\em discriminative}, meaning that they could estimate the {\em conditional} probability of an analysis given a sequence of tokens, but could not generate the tokens based on a generative model.

\subsection{A Generative Transformer-Based Model}
In order to generate the surface form (words data), there will need to be a {\em recursive tree-structured} generative model in the style of Collins' thesis parser.
The parses are latent so must be estimated with expectation maximization.
The generative probability of generating a parse is a function of 1) the logical probability according to the QBN that a given logical form would be expressed in a given linguistic context, 2) the syntactic probability that a given logical form would be expressed in a particular way.

\subsection{Syntactic Parses are Latent States}
The problem with using syntactic parses, relative to LLM's--and this is probably the reason n-gram LLM's worked before structured ones--is that one must {\em infer} a latent state.
That is, the {\em correct} syntactic analysis is ``hidden.''
This corresponds to the fact that we can {\em definitely} see what a person has written, or hear what they have said.
But, we can {\em never} be sure that we know exactly what they {\em meant}.
This is the basis for someone saying that they have been ``taken out of context.''
In other words, the allegation that one is being taken out of context is the allegation that their {\em intended} latent meaning is not the one being ascribed.
Thus, clearly, even humans cannot be sure about the {\em correct} analysis, if by ``correct'' we mean ``intended.''


\section{Future Work}
\subsection{Transferring LLM Knowledge to QBN}
In order to build full AGI, we need to combine the LLM, which has the knowledge now, to the QBN, which can store the data in a ``logically perfect'' way.
Although this is an area where litle is known, it is an area where we believe it is easy to work along the obvious dimensions.

\subsection{More Complex Logical Problems}
We would probably actually be more interested in developing a way to encode complex logical problems.

\subsection{Functions (Beyond Predicates)}
Another question is the extension to functions.
The ability for human people to do math and calculate sums and products is an interesting phenomenon.
To what extent is addition an example of {\em thinking fast}?
Well, obviousy people must use paper for complicated operations, so it must be thinking slow.
But, what about the most basic operations?
Perhaps we are equipped to compute basic calculations like $2 * 2$ fast, but $22 * 22$ requires paper or a good memory to manage those two operations.
In any event, it would seem like the most efficient option for building technology would {\em not} be to mimic the human mind exactly, since computers are better at computing.
Instead, we would probably want to break with whatever humans do for complex calculations, and instead just use the computer.
Karpathy TODO has said that in {\em OpenAI}'s strategy is to use hard-coded functions for things like math, and only use the LLM to detect when a math problem is relevant.

\section{Conclusion}
We have presented the Quanified Bayesian Network, a probabilistic graphical model that allows for 1) a generative model of the logical forms behind language, 2) that can explain its reasoning, and so not hallucinate
We have described how the QBN must be paired with a tree-structured generative model that generates the surface form (words) data from the logical form chosen.

\bibliographystyle{plain}
\bibliography{bibtex}

\end{document}


