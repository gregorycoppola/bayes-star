# BAYES STAR

## Why Should I Care About This?
This **BAYES STAR** software package implements a "QBN" which will allow **AGI**.

So, if you are interested in **AGI** you should be interested in this.

## The Quantified Bayesian Network
This software package introduces the **Quantified Bayesian Network** (**QBN**).
The QBN generalizes 1) traditional generative Bayesian Networks, and 2) First-Order Logic.

The end result, I claim, is that:
* the QBN allows a *generative* model of logical (i.e., linguistic) knowledge **that does not hallucinate**

## How Does the QBN Avoid Hallucinations?
The QBN avoids hallucinations by:
1. using logic
2. understanding how to explain its argument
3. understands that there are things it does not know

How does it do this?
1. using logic -- the QBN generalizes (though in a complex way) first-order logic
2. using ideas from classical Bayesian Networks -- allows us to create a generative story based on "causality"

## Did you Literally Build AGI?
The QBN as I am presented it is trained on **artificial data**.

It will be AGI when the QBN is trained on **real web-scale data**.

Right now, the QBN only "thinks about" very simple worlds that I encoded by hand.
But, if we assume that the LLM has "world knowledge", then the only problem to get full AGI is to transfer the knowledge from the LLM to the QBN.

That, I claim would be full AGI. Right now, I repeat, the QBN is trained on "toy universes" that I made up programmatically.

## Is it Trivial to Transfer Knowledge from LLM to QBN?
No. This is not trivial. It will require that the LLM model be re-written to generate a **tree-structured** analysis of a sentence, mapping the **surface form** of the sentence to its **logical form**.

This **logical form** is **latent**--meaning we can't observe it, and neither can actual people (this is why misunderstandings arise).

So, the following new abilities need to be developed before "full AGI" exists:
1. parse to logical forms, which are:
    a. latent (not observed)
    b. structured (recursively tree-structured)
2. concretize the continuous knowledge of the LLM into the discrete knowledge of the QBN

## Does the QBN Help us Understand the LLM?
Yes, I believe so. The QBN uses "semantic roles", which might explain why the "key-value" nature of the attention mechanism can learn world knowledge:
that is, the **key-value** knowledge of the LLM is actually learning the **semantic role** knowledge of linguistics.

## Documentation

I have the notes for a paper, but have to clean them up. So, the paper is coming.

I will be adding the informal documentation (markdown, tweets and videos) starting now. This is a "developing in public" process.

Find me online at:
* twitter: [@coppola_ai](https://twitter.com/coppola_ai)

## License

This project is licensed under the ISC License - see the [LICENSE.txt](LICENSE.txt) file for details.
